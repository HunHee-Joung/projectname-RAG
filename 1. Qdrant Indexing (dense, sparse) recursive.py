# index.py
import os
import re
import logging
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import defaultdict, Counter

import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import *
from langchain_docling import DoclingLoader
from docling.chunking import HybridChunker
from FlagEmbedding import BGEM3FlagModel

# --- Qdrant ë° ëª¨ë¸ ê³µí†µ ì„¤ì • ---
QDRANT_HOST = os.getenv("QDRANT_HOST", "192.168.0.249")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "docling_search")
MODEL_NAME = "BAAI/bge-m3"

# --- index.py ì „ìš© ì„¤ì • ---
INDEX_CONFIG = {
    "batch_size": 10,
    "max_context_length": 8192,
    "group_text_limit": 500,
    "group_total_limit": 2000,
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HybridSearchEngine:
    """ë¬¸ì„œ ì²˜ë¦¬, ì„ë² ë”©, Qdrant ì¸ë±ì‹±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, timeout=300)
        self.model = None
        self.config = INDEX_CONFIG
        logger.info(f"âœ… ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”: {QDRANT_HOST}:{QDRANT_PORT}")
    
    def _get_model(self) -> BGEM3FlagModel:
        """í•„ìš”í•  ë•Œ BGE-M3 ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not self.model:
            logger.info("ğŸ¤– BGE-M3 ëª¨ë¸ ë¡œë”©...")
            self.model = BGEM3FlagModel(MODEL_NAME, use_fp16=True)
        return self.model

    def _convert_doc_to_pdf(self, file_path: str) -> str:
        """ë¦¬ë¸Œë ˆì˜¤í”¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ DOC/DOCX íŒŒì¼ì„ PDFë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        file_path = Path(file_path)
        file_ext = file_path.suffix.lower()
        
        # DOC/DOCX íŒŒì¼ì´ ì•„ë‹ˆë©´ ì›ë³¸ ê²½ë¡œ ë°˜í™˜
        if file_ext not in ['.doc', '.docx']:
            return str(file_path)
        
        logger.info(f"ğŸ“„ DOC íŒŒì¼ PDF ë³€í™˜ ì‹œì‘: {file_path.name}")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp()
        temp_pdf_path = Path(temp_dir) / f"{file_path.stem}.pdf"
        
        try:
            # ë¦¬ë¸Œë ˆì˜¤í”¼ìŠ¤ë¥¼ ì‚¬ìš©í•œ PDF ë³€í™˜ ëª…ë ¹ì–´
            cmd = [
                "libreoffice",
                "--headless",  # GUI ì—†ì´ ì‹¤í–‰
                "--convert-to", "pdf",
                "--outdir", temp_dir,
                str(file_path)
            ]
            
            logger.info(f"ğŸ”„ ë¦¬ë¸Œë ˆì˜¤í”¼ìŠ¤ ë³€í™˜ ì‹¤í–‰: {' '.join(cmd)}")
            
            # ëª…ë ¹ì–´ ì‹¤í–‰
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                check=True
            )
            
            # ë³€í™˜ëœ PDF íŒŒì¼ í™•ì¸
            if temp_pdf_path.exists():
                # ì›ë³¸ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ ë³µì‚¬
                final_pdf_path = file_path.parent / f"{file_path.stem}.pdf"
                
                # ì´ë¯¸ ê°™ì€ ì´ë¦„ì˜ PDFê°€ ìˆë‹¤ë©´ ë®ì–´ì“°ê¸° í™•ì¸
                if final_pdf_path.exists():
                    logger.warning(f"âš ï¸  ê¸°ì¡´ PDF íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë®ì–´ì”ë‹ˆë‹¤: {final_pdf_path.name}")
                
                # PDF íŒŒì¼ ë³µì‚¬
                import shutil
                shutil.copy2(temp_pdf_path, final_pdf_path)
                
                logger.info(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {final_pdf_path.name}")
                return str(final_pdf_path)
            else:
                raise FileNotFoundError(f"ë³€í™˜ëœ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {temp_pdf_path}")
                
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ PDF ë³€í™˜ íƒ€ì„ì•„ì›ƒ: {file_path.name}")
            raise
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ PDF ë³€í™˜ ì‹¤íŒ¨: {file_path.name}")
            logger.error(f"   ì˜¤ë¥˜ ì½”ë“œ: {e.returncode}")
            logger.error(f"   í‘œì¤€ ì¶œë ¥: {e.stdout}")
            logger.error(f"   í‘œì¤€ ì—ëŸ¬: {e.stderr}")
            raise
        except Exception as e:
            logger.error(f"âŒ PDF ë³€í™˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                import shutil
                shutil.rmtree(temp_dir)
                logger.debug(f"ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {temp_dir}")
            except Exception as e:
                logger.warning(f"âš ï¸  ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def store_document(self, file_path: str) -> int:
        """ì§€ì •ëœ íŒŒì¼ì„ ë¡œë“œí•˜ê³  Qdrantì— ì¸ë±ì‹±í•˜ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info(f"ğŸ“ ë¬¸ì„œ ë¡œë”© ë° ì¸ë±ì‹± ì‹œì‘: {file_path}")
        
        # 0. DOC íŒŒì¼ì¸ ê²½ìš° PDFë¡œ ë³€í™˜
        try:
            processed_file_path = self._convert_doc_to_pdf(file_path)
            if processed_file_path != file_path:
                logger.info(f"ğŸ”„ ë³€í™˜ëœ íŒŒì¼ ì‚¬ìš©: {processed_file_path}")
        except Exception as e:
            logger.error(f"âŒ DOC íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ“„ ì›ë³¸ íŒŒì¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            processed_file_path = file_path
        
        # 1. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
        loader = DoclingLoader(file_path=processed_file_path, chunker=HybridChunker(tokenizer=MODEL_NAME, merge_peers=True, max_context_length=self.config["max_context_length"], contextualize=True))
        chunks = loader.load()
        logger.info(f"ğŸ“„ ì›ë³¸ ì²­í¬ {len(chunks)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
        # 2. ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
        if not self.client.collection_exists(collection_name=COLLECTION_NAME):
            logger.info(f"âœ¨ ìƒˆ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„± ì¤‘...")
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ìµœì í™”ëœ ì„¤ì • ì ìš©)
            self.client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config={"dense": VectorParams(size=1024, distance=Distance.COSINE, hnsw_config=HnswConfigDiff(m=16, ef_construct=100))},
                sparse_vectors_config={"sparse": SparseVectorParams()}
            )
            logger.info(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ (HNSW ìµœì í™” ì ìš©)")
            
            # í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ ìƒì„± (ìƒˆ ì»¬ë ‰ì…˜ì¸ ê²½ìš°ì—ë§Œ)
            self._create_payload_indexes()
        else:
            logger.info(f"ğŸ“¦ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì‚¬ìš©")
        
        # 3. ì‹œì‘ ID ê³„ì‚° (ê¸°ì¡´ í¬ì¸íŠ¸ ê°œìˆ˜ ê¸°ë°˜)
        collection_info = self.client.get_collection(collection_name=COLLECTION_NAME)
        start_id = collection_info.points_count
        logger.info(f"ğŸ”¢ ì‹œì‘ ID: {start_id} (ê¸°ì¡´ í¬ì¸íŠ¸: {collection_info.points_count}ê°œ)")
        
        # 4. Group ì²­í¬ ìƒì„± ë° ì „ì²´ ì²­í¬ ë³‘í•©
        group_chunks = self._create_group_chunks(chunks)
        all_chunks = chunks + group_chunks
        
        # 5. ì„ë² ë”© ë° ì—…ë¡œë“œ (ì‹œì‘ IDë¶€í„°, ì›ë³¸ íŒŒì¼ ê²½ë¡œë¡œ ì €ì¥)
        self._embed_and_upload_chunks(all_chunks, file_path, start_id)
        
        # 6. ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ 
        stats = self._collect_stats(chunks)
        self._log_results(stats, len(chunks), len(group_chunks))
        
        return len(all_chunks)
    
    def _create_payload_indexes(self):
        """í•„í„°ë§ì— ì‚¬ìš©í•  í•„ë“œì— ëŒ€í•œ í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        payload_fields = {"self_ref": PayloadSchemaType.KEYWORD, "page_no": PayloadSchemaType.INTEGER, "element_type": PayloadSchemaType.KEYWORD}
        for field, schema_type in payload_fields.items():
            logger.info(f"âš¡ï¸ '{field}' í•„ë“œì— Payload Index ìƒì„± ì¤‘...")
            self.client.create_payload_index(collection_name=COLLECTION_NAME, field_name=field, field_schema=schema_type, wait=True)
        logger.info("âœ… ëª¨ë“  Payload Index ìƒì„± ì™„ë£Œ!")
        
    def _embed_and_upload_chunks(self, chunks: List[Any], file_path: str, start_id: int = 0):
        """ì²­í¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ê³  Qdrantì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
        model = self._get_model()
        logger.info(f"ğŸš€ ì´ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {self.config['batch_size']}, ì‹œì‘ ID: {start_id})...")
        
        for i in range(0, len(chunks), self.config["batch_size"]):
            batch_chunks = chunks[i:i + self.config["batch_size"]]
            texts_to_embed = [chunk.page_content for chunk in batch_chunks]

            batch_outputs = model.encode(texts_to_embed, return_dense=True, return_sparse=True)
            
            points = []
            for j, chunk in enumerate(batch_chunks):
                meta = self._extract_metadata(chunk)
                points.append(PointStruct(
                    id=start_id + i + j,  # ì‹œì‘ IDë¶€í„° ì¦ê°€
                    vector={
                        "dense": batch_outputs["dense_vecs"][j].tolist(), 
                        "sparse": SparseVector(indices=[int(k) for k in batch_outputs["lexical_weights"][j].keys()], values=list(batch_outputs["lexical_weights"][j].values()))
                    },
                    payload={"text": chunk.page_content, "source_file": file_path, **meta}
                ))
            self.client.upsert(COLLECTION_NAME, points, wait=True)
            logger.info(f"     ... ì§„í–‰: {i + len(batch_chunks)}/{len(chunks)}ê°œ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ.")

    def _extract_metadata(self, chunk: Any) -> Dict[str, Any]:
        meta = chunk.metadata; dl_meta = meta.get('dl_meta', {}); doc_items = dl_meta.get('doc_items', [])
        result = {'page_no': None, 'bbox': None, 'element_type': 'unknown', 'headings': dl_meta.get('headings', []), 'parent_ref': None, 'self_ref': None, 'children_refs': [], 'structure_labels': [], 'is_contextualized': False}
        if not doc_items: return result
        item = doc_items[0] if isinstance(doc_items, list) else doc_items; prov = item.get('prov', [{}])[0]
        result.update({'page_no': prov.get('page_no'), 'bbox': prov.get('bbox'), 'self_ref': item.get('self_ref'),'children_refs': [c['$ref'] for c in item.get('children', []) if isinstance(c, dict) and '$ref' in c]})
        text_label = f"{item.get('self_ref', '')} {item.get('label', '')}".lower()
        if 'table' in text_label: result['element_type'] = 'table'
        elif 'figure' in text_label: result['element_type'] = 'figure'
        elif any(x in text_label for x in ['heading', 'title']): result['element_type'] = 'heading'
        else: result['element_type'] = 'text'
        if parent := item.get('parent', {}):
            if '$ref' in parent: result['parent_ref'] = parent['$ref']
        if label := item.get('label'): result['structure_labels'].append(label)
        result['is_contextualized'] = self._is_contextualized(result, chunk.page_content)
        return result
    
    def _is_contextualized(self, meta: Dict[str, Any], text: str) -> bool:
        score = ((0.4 if meta['headings'] else 0) + (0.3 if meta['parent_ref'] and meta['parent_ref'] != '#/body' else 0) + (0.2 if any(label in {'section_header','title','table','figure','heading','caption','list_item'} for label in meta['structure_labels']) else 0) + (0.1 if meta['element_type'] in {'table','figure','heading','title'} else 0))
        if len(text) <= 200 and any(re.match(p, text.strip()) for p in [r'^\d+\.\s', r'^[A-Z][A-Z\s]{2,}$', r'^Chapter\s+\d+', r'^Section\s+\d+']): score += 0.1
        return score >= 0.3
    
    def _create_group_chunks(self, chunks: List[Any]) -> List[Any]:
        groups = defaultdict(lambda: {'texts': [], 'page_nos': []})
        for chunk in chunks:
            meta = self._extract_metadata(chunk)
            if (ref := meta['parent_ref']) and ref.startswith('#/groups/'):
                groups[ref]['texts'].append(chunk.page_content[:self.config["group_text_limit"]]);
                if meta['page_no'] is not None: groups[ref]['page_nos'].append(meta['page_no'])
        group_chunks = []
        for ref, data in groups.items():
            text = " | ".join(data['texts'])
            if len(text) > self.config["group_total_limit"]: text = text[:self.config["group_total_limit"]] + "..."
            page_no = min(data['page_nos']) if data['page_nos'] else None
            chunk_obj = type('obj', (object,), {'page_content': f"Group Summary: {text}",'metadata': {'dl_meta': {'doc_items': [{'self_ref': ref, 'parent': {'$ref': '#/body'}, 'label': 'group', 'prov': [{'page_no': page_no}]}]}}})()
            group_chunks.append(chunk_obj)
        return group_chunks

    def _collect_stats(self, chunks: List[Any]) -> Dict[str, Any]:
        stats = {'contextualized': 0, 'with_headings': 0, 'with_parents': 0, 'types': Counter()}
        for chunk in chunks:
            meta = self._extract_metadata(chunk)
            if meta['is_contextualized']: stats['contextualized'] += 1
            if meta['headings']: stats['with_headings'] += 1
            if meta['parent_ref']: stats['with_parents'] += 1
            stats['types'][meta['element_type']] += 1
        return stats
    
    def _log_results(self, stats: Dict[str, Any], original: int, groups: int):
        if original == 0: logger.warning("ì²˜ë¦¬í•  ì›ë³¸ ì²­í¬ê°€ ì—†ì–´ í†µê³„ë¥¼ ìƒëµí•©ë‹ˆë‹¤."); return
        total = original + groups; ctx_ratio = stats['contextualized'] / original if original > 0 else 0
        logger.info("\n" + "="*25 + " ì¸ë±ì‹± ë¶„ì„ ê²°ê³¼ " + "="*25); logger.info(f"  - ì›ë³¸ ì²­í¬: {original}ê°œ | Group ì²­í¬: {groups}ê°œ | ì´ ì¸ë±ì‹±: {total}ê°œ"); logger.info("-" * 65); logger.info(f"  - êµ¬ì¡°í™”ëœ ì²­í¬ ë¹„ìœ¨: {stats['contextualized']}/{original} ({ctx_ratio:.1%})"); logger.info(f"  - ìš”ì†Œ íƒ€ì… ë¶„í¬: {dict(stats['types'])}")
        if ctx_ratio > 0.5: recommendation = "Context-Aware Path"
        elif ctx_ratio > 0.2: recommendation = "Hybrid Path"
        else: recommendation = "Simple Path"
        logger.info(f"  - ê²€ìƒ‰ ê²½ë¡œ ì¶”ì²œ: âœ… {recommendation}"); logger.info("="*65 + "\n")

def process_directory(engine: HybridSearchEngine, directory_path: str) -> Dict[str, Any]:
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì§€ì› íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
    SUPPORTED_EXTENSIONS = {'.pdf', '.doc', '.docx'}
    
    directory = Path(directory_path)
    if not directory.exists():
        logger.error(f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {directory_path}")
        return {"status": "error", "message": "Directory not found"}
    
    if not directory.is_dir():
        logger.error(f"âŒ ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {directory_path}")
        return {"status": "error", "message": "Path is not a directory"}
    
    # ì§€ì›ë˜ëŠ” ëª¨ë“  íŒŒì¼ ì°¾ê¸° (ì¬ê·€ì )
    all_files = []
    for ext in SUPPORTED_EXTENSIONS:
        # **/* íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ê²€ìƒ‰
        files = list(directory.rglob(f"*{ext}"))
        all_files.extend(files)
    
    if not all_files:
        logger.warning(f"âš ï¸  ì§€ì›ë˜ëŠ” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(SUPPORTED_EXTENSIONS)}")
        return {"status": "warning", "message": "No supported files found", "processed": 0, "errors": 0}
    
    logger.info(f"ğŸ“‚ ì´ {len(all_files)}ê°œì˜ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    logger.info(f"ğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {directory_path}")
    logger.info("-" * 80)
    
    # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
    results = {
        "total_files": len(all_files),
        "processed_files": 0,
        "error_files": 0,
        "total_points": 0,
        "success_files": [],
        "error_details": [],
        "start_time": datetime.now()
    }
    
    # ê° íŒŒì¼ ì²˜ë¦¬
    for idx, file_path in enumerate(all_files, 1):
        relative_path = file_path.relative_to(directory)
        logger.info(f"\nğŸ“„ [{idx}/{len(all_files)}] ì²˜ë¦¬ ì¤‘: {relative_path}")
        
        try:
            # íŒŒì¼ í¬ê¸° ì •ë³´ ì¶”ê°€
            file_size = file_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            logger.info(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
            
            # ë¬¸ì„œ ì²˜ë¦¬
            points_count = engine.store_document(str(file_path))
            
            # ì„±ê³µ ê¸°ë¡
            results["processed_files"] += 1
            results["total_points"] += points_count
            results["success_files"].append({
                "file": str(relative_path),
                "points": points_count,
                "size_mb": round(file_size_mb, 2)
            })
            
            logger.info(f"   âœ… ì™„ë£Œ: {points_count}ê°œ í¬ì¸íŠ¸ ì¶”ê°€")
            
        except Exception as e:
            # ì—ëŸ¬ ê¸°ë¡
            results["error_files"] += 1
            error_info = {
                "file": str(relative_path),
                "error": str(e),
                "error_type": type(e).__name__
            }
            results["error_details"].append(error_info)
            
            logger.error(f"   âŒ ì‹¤íŒ¨: {relative_path}")
            logger.error(f"      ì˜¤ë¥˜: {e}")
            
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ìŒ íŒŒì¼ ê³„ì† ì²˜ë¦¬
            continue
    
    # ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„ ê¸°ë¡
    results["end_time"] = datetime.now()
    results["duration"] = (results["end_time"] - results["start_time"]).total_seconds()
    
    return results

def print_processing_summary(results: Dict[str, Any]):
    """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    logger.info("\n" + "="*30 + " ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½ " + "="*30)
    logger.info(f"ğŸ“Š ì „ì²´ íŒŒì¼: {results['total_files']}ê°œ")
    logger.info(f"âœ… ì„±ê³µ: {results['processed_files']}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {results['error_files']}ê°œ")
    logger.info(f"ğŸ“ˆ ì´ ìƒì„±ëœ í¬ì¸íŠ¸: {results['total_points']:,}ê°œ")
    logger.info(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {results['duration']:.1f}ì´ˆ")
    
    if results['success_files']:
        logger.info(f"\nâœ… ì„±ê³µí•œ íŒŒì¼ë“¤:")
        for file_info in results['success_files']:
            logger.info(f"   ğŸ“„ {file_info['file']} â†’ {file_info['points']}ê°œ í¬ì¸íŠ¸ ({file_info['size_mb']}MB)")
    
    if results['error_details']:
        logger.info(f"\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for error_info in results['error_details']:
            logger.info(f"   ğŸ“„ {error_info['file']}")
            logger.info(f"      ğŸ”¸ ì˜¤ë¥˜ ìœ í˜•: {error_info['error_type']}")
            logger.info(f"      ğŸ”¸ ì˜¤ë¥˜ ë©”ì‹œì§€: {error_info['error']}")
    
    # ì„±ê³µë¥  ê³„ì‚°
    if results['total_files'] > 0:
        success_rate = (results['processed_files'] / results['total_files']) * 100
        logger.info(f"\nğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
    
    logger.info("="*75 + "\n")

if __name__ == "__main__":
    engine = HybridSearchEngine()
    data_directory = "data"
    
    logger.info(f"ğŸš€ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹œì‘: {data_directory}")
    
    try:
        # ë””ë ‰í† ë¦¬ ì „ì²´ ì²˜ë¦¬
        results = process_directory(engine, data_directory)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if results.get("status") == "error":
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {results.get('message', 'Unknown error')}")
        elif results.get("status") == "warning":
            logger.warning(f"âš ï¸  {results.get('message', 'Unknown warning')}")
        else:
            print_processing_summary(results)
            
            if results['processed_files'] > 0:
                logger.info(f"ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ! ì´ {results['processed_files']}ê°œ íŒŒì¼ì—ì„œ {results['total_points']:,}ê°œì˜ í¬ì¸íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("âš ï¸  ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                
    except KeyboardInterrupt:
        logger.warning("âš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
