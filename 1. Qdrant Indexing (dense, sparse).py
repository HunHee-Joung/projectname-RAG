# index.py
import os
import re
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import defaultdict, Counter

import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import *
from langchain_docling import DoclingLoader
from docling.chunking import HybridChunker
from FlagEmbedding import BGEM3FlagModel

# --- Qdrant ë° ëª¨ë¸ ê³µí†µ ì„¤ì • ---
QDRANT_HOST = os.getenv("QDRANT_HOST", "192.168.0.249")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "docling_search")
MODEL_NAME = "BAAI/bge-m3"


# --- index.py ì „ìš© ì„¤ì • ---
INDEX_CONFIG = {
    "batch_size": 10,
    "max_context_length": 8192,
    "group_text_limit": 500,
    "group_total_limit": 2000,
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HybridSearchEngine:
    """ë¬¸ì„œ ì²˜ë¦¬, ì„ë² ë”©, Qdrant ì¸ë±ì‹±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, timeout=300)
        self.model = None
        self.config = INDEX_CONFIG
        logger.info(f"âœ… ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”: {QDRANT_HOST}:{QDRANT_PORT}")
    
    def _get_model(self) -> BGEM3FlagModel:
        """í•„ìš”í•  ë•Œ BGE-M3 ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not self.model:
            logger.info("ğŸ¤– BGE-M3 ëª¨ë¸ ë¡œë”©...")
            self.model = BGEM3FlagModel(MODEL_NAME, use_fp16=True)
        return self.model

    # def store_document(self, file_path: str) -> int:
    #     """ì§€ì •ëœ íŒŒì¼ì„ ë¡œë“œí•˜ê³  Qdrantì— ì¸ë±ì‹±í•˜ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    #     logger.info(f"ğŸ“ ë¬¸ì„œ ë¡œë”© ë° ì¸ë±ì‹± ì‹œì‘: {file_path}")
        
    #     # 1. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
    #     loader = DoclingLoader(file_path=file_path, chunker=HybridChunker(tokenizer=MODEL_NAME, merge_peers=True, max_context_length=self.config["max_context_length"], contextualize=True))
    #     chunks = loader.load()
    #     logger.info(f"ğŸ“„ ì›ë³¸ ì²­í¬ {len(chunks)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
    #     # 2. ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
    #     if self.client.collection_exists(collection_name=COLLECTION_NAME):
    #         logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì‚­ì œ...")
    #         self.client.delete_collection(collection_name=COLLECTION_NAME)

    #     # 3. ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ìµœì í™”ëœ ì„¤ì • ì ìš©)
    #     self.client.create_collection(
    #         collection_name=COLLECTION_NAME,
    #         vectors_config={"dense": VectorParams(size=1024, distance=Distance.COSINE, hnsw_config=HnswConfigDiff(m=16, ef_construct=100))},
    #         sparse_vectors_config={"sparse": SparseVectorParams()}
    #     )
    #     logger.info(f"âœ¨ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ (HNSW ìµœì í™” ì ìš©)")
        
    #     # 4. í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ ìƒì„±
    #     self._create_payload_indexes()
        
    #     # 5. Group ì²­í¬ ìƒì„± ë° ì „ì²´ ì²­í¬ ë³‘í•©
    #     group_chunks = self._create_group_chunks(chunks)
    #     all_chunks = chunks + group_chunks
        
    #     # 6. ì„ë² ë”© ë° ì—…ë¡œë“œ
    #     self._embed_and_upload_chunks(all_chunks, file_path)
        
    #     # 7. ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ 
    #     stats = self._collect_stats(chunks)
    #     self._log_results(stats, len(chunks), len(group_chunks))
        
    #     return len(all_chunks)

    def store_document(self, file_path: str) -> int:
        """ì§€ì •ëœ íŒŒì¼ì„ ë¡œë“œí•˜ê³  Qdrantì— ì¸ë±ì‹±í•˜ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info(f"ğŸ“ ë¬¸ì„œ ë¡œë”© ë° ì¸ë±ì‹± ì‹œì‘: {file_path}")
        
        # 1. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
        loader = DoclingLoader(file_path=file_path, chunker=HybridChunker(tokenizer=MODEL_NAME, merge_peers=True, max_context_length=self.config["max_context_length"], contextualize=True))
        chunks = loader.load()
        logger.info(f"ğŸ“„ ì›ë³¸ ì²­í¬ {len(chunks)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
        # 2. ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
        if not self.client.collection_exists(collection_name=COLLECTION_NAME):
            logger.info(f"âœ¨ ìƒˆ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„± ì¤‘...")
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ìµœì í™”ëœ ì„¤ì • ì ìš©)
            self.client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config={"dense": VectorParams(size=1024, distance=Distance.COSINE, hnsw_config=HnswConfigDiff(m=16, ef_construct=100))},
                sparse_vectors_config={"sparse": SparseVectorParams()}
            )
            logger.info(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ (HNSW ìµœì í™” ì ìš©)")
            
            # í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ ìƒì„± (ìƒˆ ì»¬ë ‰ì…˜ì¸ ê²½ìš°ì—ë§Œ)
            self._create_payload_indexes()
        else:
            logger.info(f"ğŸ“¦ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì‚¬ìš©")
        
        # 3. ì‹œì‘ ID ê³„ì‚° (ê¸°ì¡´ í¬ì¸íŠ¸ ê°œìˆ˜ ê¸°ë°˜)
        collection_info = self.client.get_collection(collection_name=COLLECTION_NAME)
        start_id = collection_info.points_count
        logger.info(f"ğŸ”¢ ì‹œì‘ ID: {start_id} (ê¸°ì¡´ í¬ì¸íŠ¸: {collection_info.points_count}ê°œ)")
        
        # 4. Group ì²­í¬ ìƒì„± ë° ì „ì²´ ì²­í¬ ë³‘í•©
        group_chunks = self._create_group_chunks(chunks)
        all_chunks = chunks + group_chunks
        
        # 5. ì„ë² ë”© ë° ì—…ë¡œë“œ (ì‹œì‘ IDë¶€í„°)
        self._embed_and_upload_chunks(all_chunks, file_path, start_id)
        
        # 6. ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ 
        stats = self._collect_stats(chunks)
        self._log_results(stats, len(chunks), len(group_chunks))
        
        return len(all_chunks)
    def _create_payload_indexes(self):
        """í•„í„°ë§ì— ì‚¬ìš©í•  í•„ë“œì— ëŒ€í•œ í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        payload_fields = {"self_ref": PayloadSchemaType.KEYWORD, "page_no": PayloadSchemaType.INTEGER, "element_type": PayloadSchemaType.KEYWORD}
        for field, schema_type in payload_fields.items():
            logger.info(f"âš¡ï¸ '{field}' í•„ë“œì— Payload Index ìƒì„± ì¤‘...")
            self.client.create_payload_index(collection_name=COLLECTION_NAME, field_name=field, field_schema=schema_type, wait=True)
        logger.info("âœ… ëª¨ë“  Payload Index ìƒì„± ì™„ë£Œ!")
        
    # def _embed_and_upload_chunks(self, chunks: List[Any], file_path: str):
    #     """ì²­í¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ê³  Qdrantì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    #     model = self._get_model()
    #     logger.info(f"ğŸš€ ì´ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {self.config['batch_size']})...")
        
    #     for i in range(0, len(chunks), self.config["batch_size"]):
    #         batch_chunks = chunks[i:i + self.config["batch_size"]]
    #         texts_to_embed = [chunk.page_content for chunk in batch_chunks]

    #         batch_outputs = model.encode(texts_to_embed, return_dense=True, return_sparse=True)
            
    #         points = []
    #         for j, chunk in enumerate(batch_chunks):
    #             meta = self._extract_metadata(chunk)
    #             points.append(PointStruct(
    #                 id=i + j,
    #                 vector={
    #                     "dense": batch_outputs["dense_vecs"][j].tolist(), 
    #                     "sparse": SparseVector(indices=[int(k) for k in batch_outputs["lexical_weights"][j].keys()], values=list(batch_outputs["lexical_weights"][j].values()))
    #                 },
    #                 payload={"text": chunk.page_content, "source_file": file_path, **meta} # âœ… ColBERT í˜ì´ë¡œë“œ ì—†ìŒ
    #             ))
    #         self.client.upsert(COLLECTION_NAME, points, wait=True)
    #         logger.info(f"     ... ì§„í–‰: {i + len(batch_chunks)}/{len(chunks)}ê°œ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ.")
    
    def _embed_and_upload_chunks(self, chunks: List[Any], file_path: str, start_id: int = 0):
        """ì²­í¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ê³  Qdrantì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
        model = self._get_model()
        logger.info(f"ğŸš€ ì´ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {self.config['batch_size']}, ì‹œì‘ ID: {start_id})...")
        
        for i in range(0, len(chunks), self.config["batch_size"]):
            batch_chunks = chunks[i:i + self.config["batch_size"]]
            texts_to_embed = [chunk.page_content for chunk in batch_chunks]

            batch_outputs = model.encode(texts_to_embed, return_dense=True, return_sparse=True)
            
            points = []
            for j, chunk in enumerate(batch_chunks):
                meta = self._extract_metadata(chunk)
                points.append(PointStruct(
                    id=start_id + i + j,  # ì‹œì‘ IDë¶€í„° ì¦ê°€
                    vector={
                        "dense": batch_outputs["dense_vecs"][j].tolist(), 
                        "sparse": SparseVector(indices=[int(k) for k in batch_outputs["lexical_weights"][j].keys()], values=list(batch_outputs["lexical_weights"][j].values()))
                    },
                    payload={"text": chunk.page_content, "source_file": file_path, **meta}
                ))
            self.client.upsert(COLLECTION_NAME, points, wait=True)
            logger.info(f"     ... ì§„í–‰: {i + len(batch_chunks)}/{len(chunks)}ê°œ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ.")

    def _extract_metadata(self, chunk: Any) -> Dict[str, Any]:
        meta = chunk.metadata; dl_meta = meta.get('dl_meta', {}); doc_items = dl_meta.get('doc_items', [])
        result = {'page_no': None, 'bbox': None, 'element_type': 'unknown', 'headings': dl_meta.get('headings', []), 'parent_ref': None, 'self_ref': None, 'children_refs': [], 'structure_labels': [], 'is_contextualized': False}
        if not doc_items: return result
        item = doc_items[0] if isinstance(doc_items, list) else doc_items; prov = item.get('prov', [{}])[0]
        result.update({'page_no': prov.get('page_no'), 'bbox': prov.get('bbox'), 'self_ref': item.get('self_ref'),'children_refs': [c['$ref'] for c in item.get('children', []) if isinstance(c, dict) and '$ref' in c]})
        text_label = f"{item.get('self_ref', '')} {item.get('label', '')}".lower()
        if 'table' in text_label: result['element_type'] = 'table'
        elif 'figure' in text_label: result['element_type'] = 'figure'
        elif any(x in text_label for x in ['heading', 'title']): result['element_type'] = 'heading'
        else: result['element_type'] = 'text'
        if parent := item.get('parent', {}):
            if '$ref' in parent: result['parent_ref'] = parent['$ref']
        if label := item.get('label'): result['structure_labels'].append(label)
        result['is_contextualized'] = self._is_contextualized(result, chunk.page_content)
        return result
    
    def _is_contextualized(self, meta: Dict[str, Any], text: str) -> bool:
        score = ((0.4 if meta['headings'] else 0) + (0.3 if meta['parent_ref'] and meta['parent_ref'] != '#/body' else 0) + (0.2 if any(label in {'section_header','title','table','figure','heading','caption','list_item'} for label in meta['structure_labels']) else 0) + (0.1 if meta['element_type'] in {'table','figure','heading','title'} else 0))
        if len(text) <= 200 and any(re.match(p, text.strip()) for p in [r'^\d+\.\s', r'^[A-Z][A-Z\s]{2,}$', r'^Chapter\s+\d+', r'^Section\s+\d+']): score += 0.1
        return score >= 0.3
    
    def _create_group_chunks(self, chunks: List[Any]) -> List[Any]:
        groups = defaultdict(lambda: {'texts': [], 'page_nos': []})
        for chunk in chunks:
            meta = self._extract_metadata(chunk)
            if (ref := meta['parent_ref']) and ref.startswith('#/groups/'):
                groups[ref]['texts'].append(chunk.page_content[:self.config["group_text_limit"]]);
                if meta['page_no'] is not None: groups[ref]['page_nos'].append(meta['page_no'])
        group_chunks = []
        for ref, data in groups.items():
            text = " | ".join(data['texts'])
            if len(text) > self.config["group_total_limit"]: text = text[:self.config["group_total_limit"]] + "..."
            page_no = min(data['page_nos']) if data['page_nos'] else None
            chunk_obj = type('obj', (object,), {'page_content': f"Group Summary: {text}",'metadata': {'dl_meta': {'doc_items': [{'self_ref': ref, 'parent': {'$ref': '#/body'}, 'label': 'group', 'prov': [{'page_no': page_no}]}]}}})()
            group_chunks.append(chunk_obj)
        return group_chunks

    def _collect_stats(self, chunks: List[Any]) -> Dict[str, Any]:
        stats = {'contextualized': 0, 'with_headings': 0, 'with_parents': 0, 'types': Counter()}
        for chunk in chunks:
            meta = self._extract_metadata(chunk)
            if meta['is_contextualized']: stats['contextualized'] += 1
            if meta['headings']: stats['with_headings'] += 1
            if meta['parent_ref']: stats['with_parents'] += 1
            stats['types'][meta['element_type']] += 1
        return stats
    
    def _log_results(self, stats: Dict[str, Any], original: int, groups: int):
        if original == 0: logger.warning("ì²˜ë¦¬í•  ì›ë³¸ ì²­í¬ê°€ ì—†ì–´ í†µê³„ë¥¼ ìƒëµí•©ë‹ˆë‹¤."); return
        total = original + groups; ctx_ratio = stats['contextualized'] / original if original > 0 else 0
        logger.info("\n" + "="*25 + " ì¸ë±ì‹± ë¶„ì„ ê²°ê³¼ " + "="*25); logger.info(f"  - ì›ë³¸ ì²­í¬: {original}ê°œ | Group ì²­í¬: {groups}ê°œ | ì´ ì¸ë±ì‹±: {total}ê°œ"); logger.info("-" * 65); logger.info(f"  - êµ¬ì¡°í™”ëœ ì²­í¬ ë¹„ìœ¨: {stats['contextualized']}/{original} ({ctx_ratio:.1%})"); logger.info(f"  - ìš”ì†Œ íƒ€ì… ë¶„í¬: {dict(stats['types'])}")
        if ctx_ratio > 0.5: recommendation = "Context-Aware Path"
        elif ctx_ratio > 0.2: recommendation = "Hybrid Path"
        else: recommendation = "Simple Path"
        logger.info(f"  - ê²€ìƒ‰ ê²½ë¡œ ì¶”ì²œ: âœ… {recommendation}"); logger.info("="*65 + "\n")

if __name__ == "__main__":
    engine = HybridSearchEngine()
    sample_file = "data/sample_document.pdf"
    if Path(sample_file).exists():
        try:
            count = engine.store_document(sample_file)
            logger.info(f"ğŸ‰ ì¸ë±ì‹± ì‘ì—… ì„±ê³µ. ì´ {count}ê°œì˜ í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ì‹± ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    else:
        logger.warning(f"'{sample_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
